{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zeeshanahmad10809/covid_fake_news_classification/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (Machine Learning Techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmLZhX9iaoXB",
    "outputId": "1c38945c-0975-496c-8a7c-23da330370c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru\n",
    "!pip install tqdm\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9V9bz-PDkyA",
    "outputId": "e1d568e6-a42f-449e-a31d-549f2ef731ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from os.path import join\n",
    "from os import path\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertConfig,\n",
    ")\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, AlbertConfig\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaForSequenceClassification,\n",
    "    XLMRobertaConfig,\n",
    ")\n",
    "from transformers import (\n",
    "    ElectraTokenizer,\n",
    "    ElectraForSequenceClassification,\n",
    "    ElectraConfig,\n",
    ")\n",
    "from transformers import BartTokenizer, BartForSequenceClassification, BartConfig\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ppLiNb7GahBW"
   },
   "outputs": [],
   "source": [
    "# Set random seed values to attain deterministic behaviours\n",
    "SEED_VALUE = 19\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed_all(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "33d1RK_fbm9K"
   },
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9EqSEiptI5HX"
   },
   "outputs": [],
   "source": [
    "def transformer_params(name):\n",
    "    return {\"batch_size\": 32, \"learning_rate\": 1e-5}\n",
    "\n",
    "\n",
    "def evaluation_metrics(Y_true, Y_pred, split=\"test\"):\n",
    "    metrics = dict()\n",
    "    metrics[split + \"_accuracy\"] = accuracy_score(Y_true, Y_pred)\n",
    "    metrics[split + \"_precision\"] = precision_score(Y_true, Y_pred, average=\"macro\")\n",
    "    metrics[split + \"_recall\"] = recall_score(Y_true, Y_pred, average=\"macro\")\n",
    "    metrics[split + \"_f1_score\"] = f1_score(Y_true, Y_pred, average=\"macro\")\n",
    "    metrics[split + \"_confusion_matrix\"] = confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model, name, prev_name):\n",
    "    if not path.exists(join(os.getcwd(), \"trained\")):\n",
    "        os.mkdir(join(os.getcwd(), \"trained\"))\n",
    "    if path.exists(join(join(os.getcwd(), \"trained\"), prev_name)):\n",
    "        os.remove(join(join(os.getcwd(), \"trained\"), prev_name))\n",
    "    torch.save(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dK4Vh9mKcTzw"
   },
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    for ch in string.punctuation:\n",
    "        if ch in tweet:\n",
    "            tweet = tweet.replace(ch, \"\")\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lower_case(tweet):\n",
    "    return tweet.lower().strip()\n",
    "\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = \" \".join(lemmatizer.lemmatize(token) for token in tweet.split(\" \"))\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = lower_case(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oPg0dVoUahD7"
   },
   "outputs": [],
   "source": [
    "DATASET1 = \"COVID FakeNews Data.csv\"\n",
    "DATASET2 = \"dataset-Non-extremist-Extremist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rzQiBs9vLr0X"
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(object):\n",
    "    def __init__(self, dataset_name, mode):\n",
    "        self.dataset_name = dataset_name\n",
    "        if mode not in [\"train\", \"test\"]:\n",
    "            raise ValueError\n",
    "        self.mode = mode\n",
    "        data = None\n",
    "        try:\n",
    "            data = pd.read_csv(self.dataset_name)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"Dataset File is missing!\")\n",
    "            os._exit(0)\n",
    "        if self.dataset_name == \"dataset-Non-extremist-Extremist.csv\":\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Non-extremist\", 0)\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Extremist\", 1)\n",
    "            col_list = data.columns.to_list()\n",
    "            col_list = [col_list[-1], col_list[0]]\n",
    "            data = data[col_list]\n",
    "\n",
    "        data.iloc[:, 0] = data.iloc[:, 0].apply(preprocess_tweet)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            data.iloc[:, 0], data.iloc[:, 1], stratify=data.iloc[:, 1], test_size=0.2\n",
    "        )\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = (\n",
    "            list(self.X_train),\n",
    "            list(self.X_test),\n",
    "            list(self.y_train),\n",
    "            list(self.y_test),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.X_train)\n",
    "        else:\n",
    "            return len(self.X_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if self.mode == \"train\":\n",
    "            return self.X_train[idx], self.y_train[idx]\n",
    "        else:\n",
    "            return self.X_test[idx], self.y_test[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Fzf8elVFL35j"
   },
   "outputs": [],
   "source": [
    "def load_transformer(name):\n",
    "    logger.info(f\"Loading model {name}!\")\n",
    "    if name == \"bert-base\":\n",
    "        config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        return {\n",
    "            \"model\": BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "        }\n",
    "    elif name == \"bert-large\":\n",
    "        config = BertConfig.from_pretrained(\"bert-large-uncased\")\n",
    "        return {\n",
    "            \"model\": BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-large-uncased\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": BertTokenizer.from_pretrained(\"bert-large-uncased\"),\n",
    "        }\n",
    "    elif name == \"roberta-base\":\n",
    "        config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "        return {\n",
    "            \"model\": RobertaForSequenceClassification.from_pretrained(\n",
    "                \"roberta-base\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
    "        }\n",
    "    elif name == \"roberta-large\":\n",
    "        config = RobertaConfig.from_pretrained(\"roberta-large\")\n",
    "        return {\n",
    "            \"model\": RobertaForSequenceClassification.from_pretrained(\n",
    "                \"roberta-large\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": RobertaTokenizer.from_pretrained(\"roberta-large\"),\n",
    "        }\n",
    "    elif name == \"distilbert\":\n",
    "        config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "        return {\n",
    "            \"model\": DistilBertForSequenceClassification.from_pretrained(\n",
    "                \"distilbert-base-uncased\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\"),\n",
    "        }\n",
    "    elif name == \"albert-base-v2\":\n",
    "        config = AlbertConfig.from_pretrained(\"albert-base-v2\")\n",
    "        return {\n",
    "            \"model\": AlbertForSequenceClassification.from_pretrained(\n",
    "                \"albert-base-v2\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": AlbertTokenizer.from_pretrained(\"albert-base-v2\"),\n",
    "        }\n",
    "    elif name == \"xlmroberta-base\":\n",
    "        config = XLMRobertaConfig.from_pretrained(\"xlm-roberta-base\")\n",
    "        return {\n",
    "            \"model\": XLMRobertaForSequenceClassification.from_pretrained(\n",
    "                \"xlm-roberta-base\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\"),\n",
    "        }\n",
    "    elif name == \"electra-small\":\n",
    "        config = ElectraConfig.from_pretrained(\"google/electra-small-discriminator\")\n",
    "        return {\n",
    "            \"model\": ElectraForSequenceClassification.from_pretrained(\n",
    "                \"google/electra-small-discriminator\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": ElectraTokenizer.from_pretrained(\n",
    "                \"google/electra-small-discriminator\"\n",
    "            ),\n",
    "        }\n",
    "    elif name == \"bart-large\":\n",
    "        config = BartConfig.from_pretrained(\"facebook/bart-large\")\n",
    "        return {\n",
    "            \"model\": BartForSequenceClassification.from_pretrained(\n",
    "                \"facebook/bart-large\", config=config\n",
    "            ),\n",
    "            \"tokenizer\": BartTokenizer.from_pretrained(\"facebook/bart-large\"),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    config.num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ld1eTqMeMNv3"
   },
   "outputs": [],
   "source": [
    "def train_step(model, inputs, labels, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    labels = labels.unsqueeze(0)\n",
    "    outputs = model(\n",
    "        inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels\n",
    "    )\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "9mDYt-xdMWhP"
   },
   "outputs": [],
   "source": [
    "def eval_step(model, inputs, labels):\n",
    "    labels = labels.unsqueeze(0)\n",
    "    outputs = model(\n",
    "        inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels\n",
    "    )\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tE7AhhHjMTuP"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, tokenizer, train_dataset, optimizer, batch_size):\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    correct_count = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    with tqdm(\n",
    "        total=ceil(len(train_dataset) / batch_size), desc=\"train\", unit=\"batch\"\n",
    "    ) as pbar:\n",
    "        for text, sentiment in train_loader:\n",
    "            text = tokenizer(text, padding=True, return_tensors=\"pt\").to(device)\n",
    "            sentiment = sentiment.to(device)\n",
    "\n",
    "            logits, loss = train_step(model, text, sentiment, optimizer)\n",
    "\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "            correct_count += (preds == sentiment).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "\n",
    "    return correct_count / len(train_dataset), total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "7HyexHHrMRRm"
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, tokenizer, eval_dataset, batch_size, split):\n",
    "    eval_loader = DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    correct_count = 0\n",
    "    total_loss = 0\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            total=ceil(len(eval_dataset) / batch_size), desc=split, unit=\"batch\"\n",
    "        ) as pbar:\n",
    "            for text, sentiment in eval_loader:\n",
    "                text = tokenizer(text, padding=True, return_tensors=\"pt\").to(device)\n",
    "                sentiment = sentiment.to(device)\n",
    "\n",
    "                logits, loss = eval_step(model, text, sentiment)\n",
    "\n",
    "                preds = torch.argmax(logits, axis=1)\n",
    "                y_pred += preds.cpu().numpy().tolist()\n",
    "                y_true += sentiment.cpu().numpy().tolist()\n",
    "\n",
    "                correct_count += (preds == sentiment).sum().item()\n",
    "                total_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "    metrics_score = evaluation_metrics(y_true, y_pred, split=split)\n",
    "    return (\n",
    "        correct_count / len(eval_dataset),\n",
    "        total_loss / len(eval_dataset),\n",
    "        metrics_score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-yhhxafBMeJe"
   },
   "outputs": [],
   "source": [
    "def train(name, dataset_name, epochs=25, patience=3, save=False):\n",
    "\n",
    "    # load model and tokenizer..\n",
    "    try:\n",
    "        transformer_container = load_transformer(name)\n",
    "    except ValueError:\n",
    "        logger.error(\"Invalid transformer name!\")\n",
    "        os._exit(0)\n",
    "    model = transformer_container[\"model\"]\n",
    "    model = model.to(device)\n",
    "    tokenizer = transformer_container[\"tokenizer\"]\n",
    "\n",
    "    # load batch_size and learning rate..\n",
    "    params_container = transformer_params(name)\n",
    "    batch_size = params_container[\"batch_size\"]\n",
    "    learning_rate = params_container[\"learning_rate\"]\n",
    "\n",
    "    # load train, dev and test datasets..\n",
    "    train_dataset = SentimentDataset(dataset_name=dataset_name, mode=\"train\")\n",
    "    test_dataset = SentimentDataset(dataset_name=dataset_name, mode=\"test\")\n",
    "\n",
    "    # Intialize optimizer..\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize training variables..\n",
    "    best_acc = 0.0\n",
    "    best_loss = np.inf\n",
    "    stopping_step = 0\n",
    "    best_model_name = None\n",
    "\n",
    "    total_train_seconds = 0\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model, tokenizer, train_dataset, optimizer, batch_size\n",
    "        )\n",
    "        end = time.time()\n",
    "        total_train_seconds += end - start\n",
    "        logger.info(\n",
    "            f\"epoch: {epoch+1}, transformer: {name}, train_loss: {train_loss:.4f}, train_acc: {train_acc*100:.2f}\"\n",
    "        )\n",
    "\n",
    "        test_acc, test_loss, test_evaluation_metrics = eval_epoch(\n",
    "            model, tokenizer, test_dataset, batch_size, \"test\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"epoch: {epoch+1}, transformer: {name}, test_loss: {test_loss:.4f}, test_acc: {test_acc*100:.2f}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"epoch: {epoch+1}, transformer: {name}, \"\n",
    "            f\"test_precision: {test_evaluation_metrics['test_precision']*100:.2f}, \"\n",
    "            f\"test_recall: {test_evaluation_metrics['test_recall']*100:.2f}, \"\n",
    "            f\"test_f1_score: {test_evaluation_metrics['test_f1_score']*100:.2f}, \"\n",
    "            f\"test_accuracy_score: {test_evaluation_metrics['test_accuracy']*100:.2f}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"epoch: {epoch+1}, transformer: {name}, test_confusion_matrix: \\n\"\n",
    "            f\"{test_evaluation_metrics['test_confusion_matrix']}\"\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"Total training time elapsed: {timedelta(seconds=total_train_seconds)}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Mean time per train epoch: {timedelta(seconds=total_train_seconds/(epoch+1))}\"\n",
    "        )\n",
    "\n",
    "        # save best model and delete previous ones...\n",
    "        if save:\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                model_name = \"{}_epoch{}_model.pickle\".format(name, epoch)\n",
    "                save_model(model, model_name, best_model_name)\n",
    "\n",
    "        # Implement early stopping here\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            stopping_step = 0\n",
    "        else:\n",
    "            stopping_step += 1\n",
    "\n",
    "        if stopping_step >= patience:\n",
    "            logger.info(\"EarlyStopping!\")\n",
    "            os._exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ffNqwvbEbsYm"
   },
   "outputs": [],
   "source": [
    "CURRENT_DATASET = DATASET1\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xmm6Aaaaf5D_",
    "outputId": "106cd465-c78e-454c-dd55-4039cd0a03f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-27 08:11:14.727 | INFO     | __main__:load_transformer:2 - Loading model bert-base!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "train: 100%|██████████| 255/255 [01:23<00:00,  3.04batch/s]\n",
      "2021-02-27 08:12:45.102 | INFO     | __main__:train:38 - epoch: 1, transformer: bert-base, train_loss: 0.0053, train_acc: 95.21\n",
      "test: 100%|██████████| 64/64 [00:07<00:00,  8.63batch/s]\n",
      "2021-02-27 08:12:52.547 | INFO     | __main__:train:42 - epoch: 1, transformer: bert-base, test_loss: 0.0021, test_acc: 97.84\n",
      "2021-02-27 08:12:52.548 | INFO     | __main__:train:43 - epoch: 1, transformer: bert-base, test_precision: 94.92, test_recall: 79.35, test_f1_score: 85.34, test_accuracy_score: 97.84\n",
      "2021-02-27 08:12:52.549 | INFO     | __main__:train:48 - epoch: 1, transformer: bert-base, test_confusion_matrix: \n",
      "[[1941    5]\n",
      " [  39   56]]\n",
      "2021-02-27 08:12:52.550 | INFO     | __main__:train:51 - Total training time elapsed: 0:01:23.753923\n",
      "2021-02-27 08:12:52.551 | INFO     | __main__:train:52 - Mean time per train epoch: 0:01:23.753923\n",
      "train: 100%|██████████| 255/255 [01:28<00:00,  2.88batch/s]\n",
      "2021-02-27 08:14:20.996 | INFO     | __main__:train:38 - epoch: 2, transformer: bert-base, train_loss: 0.0022, train_acc: 97.61\n",
      "test: 100%|██████████| 64/64 [00:07<00:00,  8.41batch/s]\n",
      "2021-02-27 08:14:28.624 | INFO     | __main__:train:42 - epoch: 2, transformer: bert-base, test_loss: 0.0014, test_acc: 98.58\n",
      "2021-02-27 08:14:28.625 | INFO     | __main__:train:43 - epoch: 2, transformer: bert-base, test_precision: 96.10, test_recall: 87.24, test_f1_score: 91.15, test_accuracy_score: 98.58\n",
      "2021-02-27 08:14:28.626 | INFO     | __main__:train:48 - epoch: 2, transformer: bert-base, test_confusion_matrix: \n",
      "[[1941    5]\n",
      " [  24   71]]\n",
      "2021-02-27 08:14:28.626 | INFO     | __main__:train:51 - Total training time elapsed: 0:02:52.197648\n",
      "2021-02-27 08:14:28.627 | INFO     | __main__:train:52 - Mean time per train epoch: 0:01:26.098824\n",
      "train: 100%|██████████| 255/255 [01:30<00:00,  2.83batch/s]\n",
      "2021-02-27 08:15:58.898 | INFO     | __main__:train:38 - epoch: 3, transformer: bert-base, train_loss: 0.0013, train_acc: 98.71\n",
      "test: 100%|██████████| 64/64 [00:07<00:00,  8.22batch/s]\n",
      "2021-02-27 08:16:06.702 | INFO     | __main__:train:42 - epoch: 3, transformer: bert-base, test_loss: 0.0008, test_acc: 99.22\n",
      "2021-02-27 08:16:06.702 | INFO     | __main__:train:43 - epoch: 3, transformer: bert-base, test_precision: 97.90, test_recall: 93.08, test_f1_score: 95.35, test_accuracy_score: 99.22\n",
      "2021-02-27 08:16:06.703 | INFO     | __main__:train:48 - epoch: 3, transformer: bert-base, test_confusion_matrix: \n",
      "[[1943    3]\n",
      " [  13   82]]\n",
      "2021-02-27 08:16:06.705 | INFO     | __main__:train:51 - Total training time elapsed: 0:04:22.468330\n",
      "2021-02-27 08:16:06.705 | INFO     | __main__:train:52 - Mean time per train epoch: 0:01:27.489443\n",
      "train: 100%|██████████| 255/255 [01:32<00:00,  2.76batch/s]\n",
      "2021-02-27 08:17:39.102 | INFO     | __main__:train:38 - epoch: 4, transformer: bert-base, train_loss: 0.0007, train_acc: 99.35\n",
      "test: 100%|██████████| 64/64 [00:07<00:00,  8.14batch/s]\n",
      "2021-02-27 08:17:46.984 | INFO     | __main__:train:42 - epoch: 4, transformer: bert-base, test_loss: 0.0006, test_acc: 99.51\n",
      "2021-02-27 08:17:46.985 | INFO     | __main__:train:43 - epoch: 4, transformer: bert-base, test_precision: 97.70, test_recall: 96.74, test_f1_score: 97.21, test_accuracy_score: 99.51\n",
      "2021-02-27 08:17:46.985 | INFO     | __main__:train:48 - epoch: 4, transformer: bert-base, test_confusion_matrix: \n",
      "[[1942    4]\n",
      " [   6   89]]\n",
      "2021-02-27 08:17:46.986 | INFO     | __main__:train:51 - Total training time elapsed: 0:05:54.864327\n",
      "2021-02-27 08:17:46.987 | INFO     | __main__:train:52 - Mean time per train epoch: 0:01:28.716082\n",
      "train: 100%|██████████| 255/255 [01:31<00:00,  2.78batch/s]\n",
      "2021-02-27 08:19:18.672 | INFO     | __main__:train:38 - epoch: 5, transformer: bert-base, train_loss: 0.0006, train_acc: 99.52\n",
      "test: 100%|██████████| 64/64 [00:07<00:00,  8.02batch/s]\n",
      "2021-02-27 08:19:26.671 | INFO     | __main__:train:42 - epoch: 5, transformer: bert-base, test_loss: 0.0006, test_acc: 99.61\n",
      "2021-02-27 08:19:26.671 | INFO     | __main__:train:43 - epoch: 5, transformer: bert-base, test_precision: 99.26, test_recall: 96.29, test_f1_score: 97.72, test_accuracy_score: 99.61\n",
      "2021-02-27 08:19:26.672 | INFO     | __main__:train:48 - epoch: 5, transformer: bert-base, test_confusion_matrix: \n",
      "[[1945    1]\n",
      " [   7   88]]\n",
      "2021-02-27 08:19:26.673 | INFO     | __main__:train:51 - Total training time elapsed: 0:07:26.548399\n",
      "2021-02-27 08:19:26.674 | INFO     | __main__:train:52 - Mean time per train epoch: 0:01:29.309680\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    name=\"bert-base\",\n",
    "    dataset_name=CURRENT_DATASET,\n",
    "    epochs=EPOCHS,\n",
    "    patience=3,\n",
    "    save=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
