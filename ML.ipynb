{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5ofEVxKAw5c",
        "outputId": "a8694248-a92a-4946-847a-0012fdc27e85"
      },
      "source": [
        "!!pip install tweet-preprocessor"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfzcvwQQ9l--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c8f63f-7901-4b30-c134-a9c073f3eae4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import preprocessor as p\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_WWrONJ-xuj"
      },
      "source": [
        "SEED_VALUE = 19\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CG2O3htRzCU"
      },
      "source": [
        "DATASET1 = 'COVID FakeNews Data.csv'\n",
        "DATASET2 = 'dataset-Non-extremist-Extremist.csv'"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKMklEaOorEW"
      },
      "source": [
        "def remove_url(tweet): \n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "        \n",
        "def remove_punctuation(tweet):\n",
        "    for ch in string.punctuation:\n",
        "        if ch in tweet:\n",
        "            tweet = tweet.replace(ch, '')\n",
        "    return tweet\n",
        "\n",
        "def lower_case(tweet):\n",
        "    return tweet.lower().strip()\n",
        "\n",
        "def lemmatize(tweet):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweet = ' '.join(lemmatizer.lemmatize(token) for token in tweet.split(' '))\n",
        "  return tweet\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    #tweet = p.clean(tweet)\n",
        "    tweet = remove_url(tweet)\n",
        "    tweet = remove_punctuation(tweet)\n",
        "    tweet = lower_case(tweet)\n",
        "    tweet = lemmatize(tweet)\n",
        "    return tweet\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wof7gixlJfEU"
      },
      "source": [
        "class Dataset:\n",
        "\n",
        "  def __init__(self, dataset_name):\n",
        "    self.dataset_name = dataset_name\n",
        "    data = None\n",
        "    try:\n",
        "      data = pd.read_csv(self.dataset_name)\n",
        "    except FileNotFoundError:\n",
        "      logger.warning(\"Dataset File is missing!\")\n",
        "      os._exit(0)\n",
        "    if self.dataset_name == 'dataset-Non-extremist-Extremist.csv':\n",
        "      data['Tweet label'] = data['Tweet label'].replace('Non-extremist', 0)\n",
        "      data['Tweet label'] = data['Tweet label'].replace('Extremist', 1)\n",
        "      col_list = data.columns.to_list()\n",
        "      col_list = [col_list[-1], col_list[0]]\n",
        "      data = data[col_list]\n",
        "    \n",
        "    data.iloc[:, 0] = data.iloc[:, 0].apply(preprocess_tweet)\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(data.iloc[:, 0], data.iloc[:, 1],\n",
        "                                                    stratify=data.iloc[:, 1],\n",
        "                                                    test_size=0.2)\n",
        "    self.encoder = TfidfVectorizer()\n",
        "\n",
        "  def fit(self):\n",
        "    self.encoder.fit(self.X_train)\n",
        "    \n",
        "  def load(self):\n",
        "    self.fit()\n",
        "    return self.encoder.transform(self.X_train), self.encoder.transform(self.X_test), self.y_train, self.y_test     "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCivTPU9HxBH"
      },
      "source": [
        "class MLModel:\n",
        "\n",
        "  def __init__(self, name):\n",
        "    if name == 'logistic_regression':\n",
        "      self.model = LogisticRegression()\n",
        "    elif name == 'random_forest':\n",
        "      self.model = RandomForestClassifier()\n",
        "    elif name == 'decision_tree':\n",
        "      self.model = DecisionTreeClassifier()\n",
        "    elif name == 'svm':\n",
        "      self.model = SVC()\n",
        "    elif name == 'knn':\n",
        "      self.model = KNeighborsClassifier(n_neighbors = 5)\n",
        "    elif name == 'adaboost':\n",
        "      self.model = AdaBoostClassifier()\n",
        "    elif name == 'mlp':\n",
        "      self.model = MLPClassifier()\n",
        "    elif name == 'naive_bayes':\n",
        "      self.model = MultinomialNB()\n",
        "    else:\n",
        "      raise ValueError\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    self.model.fit(X_train, y_train)\n",
        "  \n",
        "  def predict(self, X_test):\n",
        "    return self.model.predict(X_test)\n",
        "    "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE7ruEOZQGWS"
      },
      "source": [
        "def show_performance(y_test, y_test_pred):\n",
        "  pprint(confusion_matrix(y_test, y_test_pred))\n",
        "  print(classification_report(y_test, y_test_pred, digits=4))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUbFDSf9L7EC"
      },
      "source": [
        "#Select either DATASET1 or DATASET2\n",
        "dataset = Dataset(DATASET1)\n",
        "X_train, X_test, y_train, y_test = dataset.load()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40U_9uyxQYzZ",
        "outputId": "63abce52-7676-4448-ec0b-c46ea7c30c83"
      },
      "source": [
        "#print(np.count_nonzero(y_test == 0)/len(y_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8160\n",
            "2041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVOLYaLzQheE"
      },
      "source": [
        "#Fake News Label = 0\n",
        "#Not-Fake News Label = 1\n",
        "#print(\"Fake news: \", len(data[data.outcome == 0]))\n",
        "#print(\"Not Fake news: \", len(data[data.outcome == 1]))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU3Tpfy2Q5cY"
      },
      "source": [
        "model = MLModel(name='knn')\n",
        "model.fit(X_train, y_train)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV0xxQ4-RXGj",
        "outputId": "923dae54-9977-41d2-b478-31ea0e64ef80"
      },
      "source": [
        "#USE 'macro avg'\n",
        "show_performance(y_test, y_test_pred)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[1946,    0],\n",
            "       [  59,   36]])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9706    1.0000    0.9851      1946\n",
            "           1     1.0000    0.3789    0.5496        95\n",
            "\n",
            "    accuracy                         0.9711      2041\n",
            "   macro avg     0.9853    0.6895    0.7673      2041\n",
            "weighted avg     0.9719    0.9711    0.9648      2041\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}