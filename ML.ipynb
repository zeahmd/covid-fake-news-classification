{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zeeshanahmad10809/covid_fake_news_classification/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (Machine Learning Techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5ofEVxKAw5c",
    "outputId": "a8694248-a92a-4946-847a-0012fdc27e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /home/malang/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/home/malang/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfzcvwQQ9l--",
    "outputId": "50c8f63f-7901-4b30-c134-a9c073f3eae4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/malang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n_WWrONJ-xuj"
   },
   "outputs": [],
   "source": [
    "# Set random seed values to attain deterministic behaviours\n",
    "SEED_VALUE = 19\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fKMklEaOorEW"
   },
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    for ch in string.punctuation:\n",
    "        if ch in tweet:\n",
    "            tweet = tweet.replace(ch, \"\")\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lower_case(tweet):\n",
    "    return tweet.lower().strip()\n",
    "\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = \" \".join(lemmatizer.lemmatize(token) for token in tweet.split(\" \"))\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = lower_case(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8CG2O3htRzCU"
   },
   "outputs": [],
   "source": [
    "DATASET1 = \"COVID FakeNews Data.csv\"\n",
    "DATASET2 = \"dataset-Non-extremist-Extremist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wof7gixlJfEU"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        data = None\n",
    "        try:\n",
    "            data = pd.read_csv(self.dataset_name)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"Dataset File is missing!\")\n",
    "            os._exit(0)\n",
    "        if self.dataset_name == \"dataset-Non-extremist-Extremist.csv\":\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Non-extremist\", 0)\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Extremist\", 1)\n",
    "            col_list = data.columns.to_list()\n",
    "            col_list = [col_list[-1], col_list[0]]\n",
    "            data = data[col_list]\n",
    "\n",
    "        data.iloc[:, 0] = data.iloc[:, 0].apply(preprocess_tweet)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            data.iloc[:, 0], data.iloc[:, 1], stratify=data.iloc[:, 1], test_size=0.2\n",
    "        )\n",
    "        self.encoder = TfidfVectorizer()\n",
    "\n",
    "    def fit(self):\n",
    "        self.encoder.fit(self.X_train)\n",
    "\n",
    "    def load(self):\n",
    "        self.fit()\n",
    "        return (\n",
    "            self.encoder.transform(self.X_train),\n",
    "            self.encoder.transform(self.X_test),\n",
    "            self.y_train,\n",
    "            self.y_test,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lCivTPU9HxBH"
   },
   "outputs": [],
   "source": [
    "class MLModel:\n",
    "    def __init__(self, name):\n",
    "        if name == \"logistic_regression\":\n",
    "            self.model = LogisticRegression()\n",
    "        elif name == \"random_forest\":\n",
    "            self.model = RandomForestClassifier()\n",
    "        elif name == \"decision_tree\":\n",
    "            self.model = DecisionTreeClassifier()\n",
    "        elif name == \"svm\":\n",
    "            self.model = SVC()\n",
    "        elif name == \"knn\":\n",
    "            self.model = KNeighborsClassifier(n_neighbors=5)\n",
    "        elif name == \"adaboost\":\n",
    "            self.model = AdaBoostClassifier()\n",
    "        elif name == \"mlp\":\n",
    "            self.model = MLPClassifier()\n",
    "        elif name == \"naive_bayes\":\n",
    "            self.model = MultinomialNB()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DUbFDSf9L7EC"
   },
   "outputs": [],
   "source": [
    "# Select either DATASET1 or DATASET2\n",
    "dataset = Dataset(DATASET1)\n",
    "X_train, X_test, y_train, y_test = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40U_9uyxQYzZ",
    "outputId": "63abce52-7676-4448-ec0b-c46ea7c30c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8160\n",
      "2041\n"
     ]
    }
   ],
   "source": [
    "# print(np.count_nonzero(y_test == 0)/len(y_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QVOLYaLzQheE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fake news:8160,\\Test fake news: 2041\n",
      "Train non-fake news: 8160, Test non-fake news: 2041\n"
     ]
    }
   ],
   "source": [
    "# Fake News Label = 0\n",
    "# Not-Fake News Label = 1\n",
    "print(f\"Train fake news:{len(y_train == 0)},\\Test fake news: {len(y_test == 0)}\")\n",
    "print(f\"Train non-fake news: {len(y_train == 1)}, Test non-fake news: {len(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aU3Tpfy2Q5cY"
   },
   "outputs": [],
   "source": [
    "model = MLModel(name=\"knn\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IE7ruEOZQGWS"
   },
   "outputs": [],
   "source": [
    "def show_performance(y_test, y_test_pred):\n",
    "    pprint(confusion_matrix(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV0xxQ4-RXGj",
    "outputId": "923dae54-9977-41d2-b478-31ea0e64ef80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1946,    0],\n",
      "       [  59,   36]])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9706    1.0000    0.9851      1946\n",
      "           1     1.0000    0.3789    0.5496        95\n",
      "\n",
      "    accuracy                         0.9711      2041\n",
      "   macro avg     0.9853    0.6895    0.7673      2041\n",
      "weighted avg     0.9719    0.9711    0.9648      2041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USE 'macro avg'\n",
    "show_performance(y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
