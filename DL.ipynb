{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/zeeshanahmad10809/covid_fake_news_classification/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPd1FfPhrU2B",
    "outputId": "6f365e7c-03ab-49e6-f68f-810c2b4bc62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
      "\r",
      "\u001b[K     |█████▊                          | 10kB 18.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 20kB 21.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 30kB 16.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 40kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 51kB 9.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s \n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.5.3\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_BKj9TGVLDs",
    "outputId": "0f23db5b-8870-42d7-8c8f-b8b50bee833f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-27 07:52:56--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1523785255 (1.4G) [application/zip]\n",
      "Saving to: ‘crawl-300d-2M.vec.zip’\n",
      "\n",
      "crawl-300d-2M.vec.z 100%[===================>]   1.42G  53.4MB/s    in 26s     \n",
      "\n",
      "2021-02-27 07:53:22 (55.5 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
      "\n",
      "Archive:  crawl-300d-2M.vec.zip\n",
      "  inflating: crawl-300d-2M.vec       \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
    "!unzip crawl-300d-2M.vec.zip\n",
    "!rm crawl-300d-2M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBViJbJ-W0_y",
    "outputId": "48a21a68-5d69-46b9-d0ac-c4ba94872d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    SimpleRNN,\n",
    "    Conv1D,\n",
    "    GlobalMaxPool1D,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CFQGz6nQD6xc"
   },
   "outputs": [],
   "source": [
    "SEED_VALUE = 19\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "tf.random.set_seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dgxuJ7w0yN7l"
   },
   "outputs": [],
   "source": [
    "DATASET1 = \"COVID FakeNews Data.csv\"\n",
    "DATASET2 = \"dataset-Non-extremist-Extremist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cV8_0e-ucLu8"
   },
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    return \" \".join(\n",
    "        re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    for ch in string.punctuation:\n",
    "        if ch in tweet:\n",
    "            tweet = tweet.replace(ch, \"\")\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lower_case(tweet):\n",
    "    return tweet.lower().strip()\n",
    "\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = \" \".join(lemmatizer.lemmatize(token) for token in tweet.split(\" \"))\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # tweet = p.clean(tweet)\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = lower_case(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oizUVH7HrcN0"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        data = None\n",
    "        try:\n",
    "            data = pd.read_csv(self.dataset_name)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(\"Dataset File is missing!\")\n",
    "            os._exit(0)\n",
    "        if self.dataset_name == \"dataset-Non-extremist-Extremist.csv\":\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Non-extremist\", 0)\n",
    "            data[\"Tweet label\"] = data[\"Tweet label\"].replace(\"Extremist\", 1)\n",
    "            col_list = data.columns.to_list()\n",
    "            col_list = [col_list[-1], col_list[0]]\n",
    "            data = data[col_list]\n",
    "\n",
    "        data.iloc[:, 0] = data.iloc[:, 0].apply(preprocess_tweet)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            data.iloc[:, 0], data.iloc[:, 1], stratify=data.iloc[:, 1], test_size=0.2\n",
    "        )\n",
    "        self.embedding_size = 300\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def get_embedding_size(self):\n",
    "        return self.embedding_size\n",
    "\n",
    "    def get_word_index(self):\n",
    "        return self.tokenizer.word_index\n",
    "\n",
    "    def create_vocab(self):\n",
    "        self.words_set = set()\n",
    "        self.max_sentence_len = 0\n",
    "        for sentence in self.X_train:\n",
    "            tokens = sentence.split(\" \")\n",
    "            if len(tokens) > self.max_sentence_len:\n",
    "                self.max_sentence_len = len(tokens)\n",
    "            for word in tokens:\n",
    "                self.words_set.add(word)\n",
    "\n",
    "    def get_vocab_info(self):\n",
    "        return len(self.words_set), self.max_sentence_len\n",
    "\n",
    "    def fit(self):\n",
    "        self.create_vocab()\n",
    "        vocab_size, max_sentence_len = self.get_vocab_info()\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        self.tokenizer.fit_on_texts(self.X_train)\n",
    "\n",
    "    def load(self):\n",
    "        self.fit()\n",
    "        self.X_train = sequence.pad_sequences(\n",
    "            self.tokenizer.texts_to_sequences(self.X_train),\n",
    "            maxlen=self.max_sentence_len,\n",
    "            padding=\"post\",\n",
    "        )\n",
    "        self.X_test = sequence.pad_sequences(\n",
    "            self.tokenizer.texts_to_sequences(self.X_test),\n",
    "            maxlen=self.max_sentence_len,\n",
    "            padding=\"post\",\n",
    "        )\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3PCR7hjUszVI"
   },
   "outputs": [],
   "source": [
    "def loadFastTextModel(path=\"\"):\n",
    "    logger.info(\"Loading FastText Model!\")\n",
    "    embeddings_index = dict()\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            with tqdm(total=1999996, desc=\"loading FastText\") as pbar:\n",
    "                for line in f:\n",
    "                    values = line.strip().split(\" \")\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "                    embeddings_index[word] = coefs\n",
    "                    pbar.update(1)\n",
    "\n",
    "        return embeddings_index\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Embedding file not in path!\")\n",
    "        os._exit(0)\n",
    "\n",
    "\n",
    "def buildEmbeddingMatrix(word_index, vocab_size, embedding_size, embeddings_index):\n",
    "    logger.info(\"Building Embedding Matrix!\")\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0pZhA89Uw-Kp"
   },
   "outputs": [],
   "source": [
    "def LSTM_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Dropout(0.1),\n",
    "            LSTM(64),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def GRU_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    initializer = tf.keras.initializers.GlorotUniform()\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            GRU(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_initializer=initializer,\n",
    "                recurrent_initializer=initializer,\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            GRU(64, kernel_initializer=initializer, recurrent_initializer=initializer),\n",
    "            Dropout(0.3),\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer,\n",
    "                name=\"relu_dense1\",\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def RNN_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    initializer = tf.keras.initializers.GlorotUniform()\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            SimpleRNN(\n",
    "                64,\n",
    "                return_sequences=True,\n",
    "                kernel_initializer=initializer,\n",
    "                recurrent_initializer=initializer,\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            SimpleRNN(\n",
    "                64, kernel_initializer=initializer, recurrent_initializer=initializer\n",
    "            ),\n",
    "            Dropout(0.3),\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=initializer,\n",
    "                name=\"relu_dense1\",\n",
    "            ),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def Conv1d_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            Conv1D(128, 3, strides=1, padding=\"SAME\", activation=\"relu\"),\n",
    "            Dropout(0.1),\n",
    "            Conv1D(256, 3, strides=1, padding=\"SAME\", activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            GlobalMaxPool1D(),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def BiLSTM_Model(weights, vocab_size, embedding_size, max_sen_len, num_classes):\n",
    "    return Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                weights=[weights],\n",
    "                trainable=False,\n",
    "                input_shape=(max_sen_len,),\n",
    "            ),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            Dropout(0.1),\n",
    "            Bidirectional(LSTM(64)),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\", name=\"relu_dense1\"),\n",
    "            Dropout(0.1),\n",
    "            Dense(num_classes, activation=\"sigmoid\", name=\"sigmoid_dense\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def buildModel(\n",
    "    name=\"lstm\",\n",
    "    embedding_matrix=None,\n",
    "    vocab_size=16222,\n",
    "    embedding_size=None,\n",
    "    max_sen_len=56,\n",
    "    num_classes=2,\n",
    "):\n",
    "    if name == \"lstm\":\n",
    "        return LSTM_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"gru\":\n",
    "        return GRU_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"rnn\":\n",
    "        return RNN_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"bilstm\":\n",
    "        return BiLSTM_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    elif name == \"conv1d\":\n",
    "        return Conv1d_Model(\n",
    "            embedding_matrix, vocab_size, embedding_size, max_sen_len, num_classes - 1\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Invalid model name {name}\")\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_UdozvXn7PrF"
   },
   "outputs": [],
   "source": [
    "def show_performance(y_test, y_test_pred):\n",
    "    pprint(confusion_matrix(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T535XUQOwTef",
    "outputId": "ca1b5b28-d3ca-4bab-c642-8249c3f7e6b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-27 07:58:14.372 | INFO     | __main__:loadFastTextModel:2 - Loading FastText Model!\n",
      "loading FastText: 100%|██████████| 1999996/1999996 [02:07<00:00, 15702.83it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_embeddings_index = loadFastTextModel(\"crawl-300d-2M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39VcOPt2wcWl",
    "outputId": "2dec7fd8-60cc-4800-da71-a608be9b9f7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-27 08:01:46.705 | INFO     | __main__:buildEmbeddingMatrix:22 - Building Embedding Matrix!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(DATASET2)\n",
    "X_train, X_test, y_train, y_test = dataset.load()\n",
    "vocab_size, max_sentence_len = dataset.get_vocab_info()\n",
    "word_index = dataset.get_word_index()\n",
    "embedding_size = dataset.get_embedding_size()\n",
    "embedding_matrix = buildEmbeddingMatrix(\n",
    "    word_index, vocab_size, embedding_size, fasttext_embeddings_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CwFj-UtO0iS7"
   },
   "outputs": [],
   "source": [
    "model = buildModel(\n",
    "    name=\"lstm\",\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    max_sen_len=max_sentence_len,\n",
    "    num_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eCtHziPb5EBB"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02zVLx9f5Z6G",
    "outputId": "b0d1ceda-40ce-45b9-a0c7-9695d7a6b099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.4767 - accuracy: 0.7759 - val_loss: 0.4978 - val_accuracy: 0.7489\n",
      "Epoch 2/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.4579 - accuracy: 0.7834 - val_loss: 0.4875 - val_accuracy: 0.7716\n",
      "Epoch 3/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.4379 - accuracy: 0.7947 - val_loss: 0.4948 - val_accuracy: 0.7666\n",
      "Epoch 4/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.4237 - accuracy: 0.7988 - val_loss: 0.4715 - val_accuracy: 0.7688\n",
      "Epoch 5/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.4056 - accuracy: 0.8116 - val_loss: 0.4782 - val_accuracy: 0.7652\n",
      "Epoch 6/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.3889 - accuracy: 0.8193 - val_loss: 0.4755 - val_accuracy: 0.7688\n",
      "Epoch 7/15\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.3705 - accuracy: 0.8292 - val_loss: 0.5010 - val_accuracy: 0.7683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f43040af0d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DSXNGSnI5ima"
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.where(model.predict(X_test) < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOh6Pcn27JCl",
    "outputId": "54f9c96d-4663-4e9f-cc19-4e5862ed6cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1008,  679],\n",
      "       [ 303, 2248]])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7689    0.5975    0.6724      1687\n",
      "           1     0.7680    0.8812    0.8207      2551\n",
      "\n",
      "    accuracy                         0.7683      4238\n",
      "   macro avg     0.7685    0.7394    0.7466      4238\n",
      "weighted avg     0.7684    0.7683    0.7617      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USE 'macro avg'\n",
    "show_performance(y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
